[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /opt/airflow/logs

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = LocalExecutor

# The SQLAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The encoding used to decode output from bash commands
# output_encoding = utf-8

[logging]
# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

[scheduler]
# Task instances are picked up by the scheduler from this directory
child_process_log_directory = /opt/airflow/logs/scheduler

# The scheduler can run multiple threads in parallel. This defines how many.
max_threads = 2

[webserver]
# The base url of your airflow installation
# base_url = http://localhost:8080

# The ip specified here is the ip of the webserver
# web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

[database]
# The sqlalchemy connection string to the metadata database.
# This string can be changed to point to a different database backend
# sql_alchemy_conn = sqlite:////opt/airflow/airflow.db
